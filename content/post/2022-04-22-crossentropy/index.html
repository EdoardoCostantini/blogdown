---
title: Cross-entropy as a measure of predictive accuracy
draft: true # true
author: Edoardo Costantini
date: '2022-04-22'
slug: cross-entropy
categories: ["Tutorials", "Drafts"]
tags: ["prediction", "machine learning"]
subtitle: ''
summary: ''
authors: []
lastmod: '2022-04-22T16:12:58+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
    number_sections: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#learn-by-coding"><span class="toc-section-number">2</span> Learn by coding</a></li>
<li><a href="#tldr-just-give-me-the-code"><span class="toc-section-number">3</span> TL;DR, just give me the code!</a></li>
<li><a href="#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>

<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Cross-entropy quantifies the difference between two probability distributions.
As such, it comes in handy as a <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> in multi-class classification tasks (e.g., multinomial logistic regression).
Cross-entropy provides an elegant solution for determining the difference between actual and predicted categorical data point values.</p>
</div>
<div id="learn-by-coding" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Learn by coding</h1>
<p>Bla bla</p>
</div>
<div id="tldr-just-give-me-the-code" class="section level1" number="3">
<h1><span class="header-section-number">3</span> TL;DR, just give me the code!</h1>
</div>
<div id="references" class="section level1" number="4">
<h1><span class="header-section-number">4</span> References</h1>
<p>List of other resources:</p>
<ul>
<li><a href="https://rpubs.com/juanhklopper/cross_entropy">Cross-entropy in RPubs</a></li>
<li><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">A Gentle Introduction to Cross-Entropy for Machine Learning</a></li>
<li><a href="https://gombru.github.io/2018/05/23/cross_entropy_loss/">Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names</a></li>
</ul>
</div>
