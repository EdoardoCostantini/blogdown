---
title: The sweep operator
author: Edoardo Costantini
date: '2021-11-17'
slug: sweep
categories: ["Tutorials", "Drafts"]
tags: ["statistics", "regression"]
subtitle: ''
summary: ''
authors: []
lastmod: '2022-04-02T15:33:01+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: references.bib
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
    number_sections: true
---

```{r set up, echo = FALSE, results = 'hide', message = FALSE, warning = FALSE}

# Packages and functions
source("sweepGoodnight.R") # main sweep function
library(ISR3)                  # for SWP functions
library(fastmatrix)            # alternative sweep

# Take the mtcars data
y <- mtcars[, "mpg"]
X <- mtcars[, -1]

# Create a few shorthands we will use
n <- nrow(X)
p <- ncol(X)

```

# Introduction

The sweep operator is a matrix transformation commonly used to estimate regression models.
It performs elementary row operations on a $p \times p$ matrix which happen to be particularly useful for the estimation of multivariate linear models.
Little and Rubin [-@littleRubin:2002 p148] defined it as follows:

> The sweep operator is defined for symmetric matrices as follows. A $p \times p$ symmetric matrix G is said to be swept on row and column k if it is replaced by another symmetric $p \times p$ matrix H with elements defined as follows:
> $$
> h_{kk} = -1/g_{kk}
> $$
> $$
> h_{jk} = h_{kj} = \frac{g_{jk}}{g_{kk}}, j \neq k
> $$
> $$
> h_{jl} = g_{jl} - \frac{g_{jk}g_{kl}}{g_{kk}}, j \neq k, l \neq k
> $$

The notation indicating this transformation is usually a variation of SWEEP(k)[G], which can be read as sweeping matrix G on row and column k.

In this post, I'm interested in exploring how we use the sweep operator to estimate the parameters of regressions models.
If you are interested in the mathematical details, I recommend reading the full sweep operator description in one of the following resources: Goodnight [-@goodnight:1979 p154], Schafer [-@schafer:1997], Little and Rubin [-@littleRubin:2002 p148].

Goodnight [-@goodnight:1979 p150] is a particularly helpful paper as it describes an easy to implement algorithm to perform the sweep operator.
Following Goodnight, given an originally symmetric positive definite matrix G, SWEEP(k)[G] modifies a matrix G as follows:

- Step 1: Let $D = g_{kk}$
- Step 2: Divide row $k$ by $D$.
- Step 3: For every other row $i \neq k$, let $B = g_{ik}$. Sub-tract $B \times$ row $k$ from row $i$. Set $g_{ik} = -B/D$.
- Step 4: Set $g_{kk} = 1/D$.

# Learn by coding

## Coding a sweep function in R

Let's start by coding a simple function that performs the operations described by Goodnight [-@goodnight:1979 p150].
We want a function that takes as inputs a symmetric matrix (argument `G`) and a vector of positions to sweep over (argument `K`).
The function below takes these two inputs and performs the four sweep steps for every element of `K`.

```{r sweep function}
# Write an R function implementing SWEEP(k)[G] according to Goodnight ----------

sweepGoodnight <- function (G, K){

  for(k in K){
    # Step 1: Let D = g_kk
    D <- G[k, k]

    # Step 2: Divide row k by D.
    G[k, ] <- G[k, ] / D

    # Step 3:
    # - For every other row i != k, let B = g_ik
    # - Subtract B \times row k from row i.
    # - set g_ik = -B/D.
    for(i in 1:nrow(G)){
      if(i != k){
        B <- G[i, k]
        G[i, ] <- G[i, ] - B * G[k, ]
        G[i, k] <- -1 * B / D
      }
    }
    # Step 4: Set g_kk = 1/D
    G[k, k] = 1/D
  }

  # Output
  return(G)
}

```

Let's check that this function returns what we want by comparing it with a function implemented by someone else.

```{r compare sweeps, results = 'hide'}
# Compare sweepGoodnight with other implementations ----------------------------

# Install the `fastmatrix` package (run if you don't have it yet)
# install.packages("fastmatrix")

# Load fastmatrix
library(fastmatrix)

# Define an example dataset
X <- matrix(c(1, 1, 1, 1,
              1, 2, 1, 3,
              1, 3, 1, 3,
              1, 1,-1, 2,
              1, 2,-1, 2,
              1, 3,-1, 1), ncol = 4, byrow = TRUE)

# Define the G matrix
G <- crossprod(X)

# Define a vector of position to sweep over
K <- 1:3

# Perform Sweep[K](G) with fastmatrix sweep.operator
H_fm <- sweep.operator(G, k = K)

# Perform Sweep[K](G) with our sweepGoodnight implementation
H_sg <- sweepGoodnight(G, K = K)

# Compare the two
H_fm - H_sg

```

You will notice that `H_fm - H_sg` returns a $4 \times 4$ matrix of 0s: mission accomplished.

## Using the sweep operator to estimate regression models

To understand the sweep operator we will use it to compute the regression coefficient of a multivariate linear model estimated on an example dataset.
For this post, we will work with the data used by Little and Rubin [-@littleRubin:2002 p152].

```{r load data}
# Load Little Rubin data -------------------------------------------------------

# Create data
  X <- as.data.frame(
          matrix(
                  data = c(7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10, 26,
                           29, 56, 31, 52, 55, 71 ,31, 54, 47, 40, 66, 68,
                           6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8,
                           60, 52, 20, 47, 33, 22,6,44,22,26,34,12,12,
                           78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7,
                           72.5, 93.1, 115.9, 83.8, 113.3, 109.4),
                  ncol = 5
          )
  )

# Store useful information
  n <- nrow(X)
  p <- ncol(X)

```

Let's take a quick look at the first rows of the data to get an idea of what we are working with.

```{r check X}
# Glace at the first 6 rows of the data
  head(X)

```

### Compute the augmented covariance matrix

The sweep operator gets us the regression coefficients of a multivairate linear model when applied to what is known as the augmented covariance matrix of the data ($\Theta$).
This is a $(p+1) \times (p+1)$ matrix storing the covariance matrix and the means of the dataset.
It usually looks like this:

$$
\Theta =
\begin{bmatrix}
-1 & \mu_1 & ... &\mu_p\\
\mu_1 & \sigma^2_1 & ... & \sigma_{1p}\\
... & ... & ... & ...\\
\mu_p & \sigma_{1p} & ... & \sigma^2_{p}
\end{bmatrix}
$$

with $\mu_1, \dots, \mu_p$, $\sigma^2_1, \dots, \sigma^2_p$, and $\sigma_{jk}$ being the means, variances and covariances of the variables in our dataset, respectively.

In R, we can obtain this matrix in just a few steps starting from our dataset `X`:

- **Augment the original data** with a column of 1s on the left
  ```{r augment X}
  # Obtain the augmented covariance matrix ---------------------------------------

  # Augment X
    X_aug <- cbind(int = 1, as.matrix(X))

  # Glance at the first 6 rows of X_aug
    head(X_aug)

  ```

- Compute the **augmented matrix of sufficient statistics $T$**.

  $T$ is the matrix having as elements the sum of the cross-products of the columns of `X_aug`.
  Since the first column of `X_aug` is a column of 1s, the first element of T is the number of rows in the data, the first column and rows store the sum of scores on each variable (sufficient statistics for the mean), and the other elements store the sum of products between the columns of `X` (sufficient statistics for the covariance matrix of `X`).

  $$
  T =
  \begin{bmatrix}
  n & \sum{x_1} & ... & \sum{x_p}\\
  \sum{x_1} & \sum{x_1^2} & ... & \sum{x_1 x_p}\\
  ... & ... & ... & ...\\
  \sum{x_p} & \sum{x_1 x_p} & ... & \sum{x_p^2}
  \end{bmatrix}
  $$
  In R, we can compute it easily with the cross-product function

  ```{r compute T}
  # Compute the matrix of sufficient statistics (T matrix)
    Tmat <- crossprod(X_aug)

  ```

- **Transform T to G**

  $G$ is simply $T / n$

  $$
  G =
  \begin{bmatrix}
   1 & \mu_1 & ... &\mu_p\\
   \mu_1 & \frac{\sum{x_1^2}}{n} & ... & \frac{\sum{x_1 x_p}}{n}\\
   ... & ... & ... & ...\\
   \mu_p & \frac{\sum{x_1 x_p}}{n} & ... & \frac{\sum{x_p^2}}{n}
  \end{bmatrix}
  $$

  ```{r compute G}
  # Compute G
    G <- Tmat / n

  ```

- **Compute $\Theta$** by sweeping G over the first row and column.

  Let's use our `sweepGoodnight()` function to perform SWEEP(1)[G] and obtain $\Theta$

  $$
  \Theta =
  \begin{bmatrix}
  -1 & \mu_1 & ... &\mu_p\\
  \mu_1 & \sigma^2_1 & ... & \sigma_{1p}\\
  ... & ... & ... & ...\\
  \mu_p & \sigma_{1p} & ... & \sigma^2_{p}
  \end{bmatrix}
  $$

  In R:
  ```{r compute Theta}
  # Sweep G over the first position
    Theta <- sweepGoodnight(G, 1)

  # Check how it looks
    Theta

  # Check Theta is storing the means in the first row and column
    colMeans(X)

  # Check Theta is storing the ML covariance matrix everywhere else
    cov(X) * (n-1) / n

  ```

  Pay attention to a few of things:
  - The covariance matrix store in $\Theta$ is the Maximum Likelihood version (denominator should be `n` instead of the default `n-1`)
  - The first element of the `Theta` object is negative, which indicates the matrix has been swept over that row and column.
  - We could have constructed the object `Theta` just by using `colMeans(X)` and `cov(X) * (n-1) / n` directly.
    However, it is important to note the relationship between `Tmat`, `G`, and `Theta`.
    In particular, pay attention to the fact that `Theta` is the result of sweeping `G` in the first position.
    When I started looking into this topic I did not understand this, and I kept sweeping `Theta` over the first position, resulting in a confusing double sweep of the first column and row.
    I will get back to this point in a sec.

### Estimate multivariate linear models

Now let's see how we can use $\Theta$ to estimate any multivariate linear model involving the variables in our dataset.
First, let's see how we would obtain these linear models in R with standard procedures.
Say we want to regress V1 and V3 on V2, V4, and V5.
We will start by creating a formula for an `lm` function return the model we want.

```{r mlm definition}
# Fit some multivariate linear models ------------------------------------------

  # Define the dependent variables (dvs) of the multivairate linear models
  dvs <- c("V1", "V3")

  # Define the predictors (ivs) of the multivairate linear models
  ivs <- c("V2", "V4", "V5")

  # Create the formula (complicated but flexible way)
  formula_lm <- paste0("cbind(",
                       paste0(dvs, collapse = ", "),
                       ") ~ ",
                       paste0(ivs, collapse = " + "))

  # Check the formula
  formula_lm

```

  Next,

```{r mlm estimation}
  # Fit the model with the MLM
  mlm0 <- lm(formula_lm, data = X)
  coef(mlm0)

```

  These are our intercepts, and regression coefficients for the multivariate linear model.
  Given a $\Theta$ matrix, we can sweep on the positions of the independent variables to obtain the regression coefficients of all the multivariate linear model with the variables we did not sweep over as dependent variables.
  First, let's define a vector of positions to sweep over based on the variable names we stored in `ivs`.

```{r define sweep over}
# Fit some multivariate linear models using sweep ------------------------------

  # Define positions to sweep over
  sweep_over <- which(colnames(G) %in% ivs)

```

Then, let's simply sweep our $\Theta$ over these positions.

```{r sweep theta}
  # Sweep theta
  H <- sweepGoodnight(Theta, K = sweep_over)

  # Check out the result
  H

```

Our regression coefficients are here in this new matrix.
We just need to find them.
We know that the dependent variables are V1 and V3, and that the independent variables are V2, V4, and V5.
Let's index the columns of `H` with the names of the dvs and the rows with the names of the ivs.

```{r index H}
  # Extract the regression coefficients from H
  H[c("int", ivs), dvs]

  # Compare with coefficients from lm function
  coef(mlm0)

```

Note that, ww are sweeping $\Theta$ only over the predictors, but we also get the estimate of the intercept.
However, remember that $\Theta$ is the result of sweeping G over the first position, which is the position where the intercept estimate appears.
You could obtain the same result by directly sweeping G over position 1, and the position of the predictors.
In code:

```{r sweeping G as theta}
  # Sweep G
  sweepGoodnight(G, c(1, sweep_over))[c("int", ivs), dvs]

  # Sweep theta (notice we do not sweep over 1)
  sweepGoodnight(Theta, sweep_over)[c("int", ivs), dvs]

```

Finally, just play around with what variables you consider as dvs and ivs.
You will discover the magic of the sweep operator.

```{r variable roles}
# Play around with variable roles ------------------------------------------

  # Define different dependent variables (dvs) for the multivairate linear models
  dvs <- c("V1", "V2", "V5")

  # Define different predictors (ivs) for the multivairate linear models
  ivs <- c("V3", "V4")

  # Create the formula (complicated but flexible way)
  formula_lm <- paste0("cbind(",
                       paste0(dvs, collapse = ", "),
                       ") ~ ",
                       paste0(ivs, collapse = " + "))

  # Fit the model with the MLM
  mlm1 <- lm(formula_lm, data = X)
  coef(mlm1)

  # Define positions to sweep over
  sweep_over <- which(colnames(Theta) %in% ivs)

  # Sweep Theta over new positions
  sweepGoodnight(Theta, K = sweep_over)[c("int", ivs), dvs]

```

# TL;DR, just give me the code!
```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```

# References