---
title: Cross-entropy as a measure of predictive accuracy
draft: false # true
author: Edoardo Costantini
date: '2022-04-22'
slug: cross-entropy
categories: ["Drafts", "Tutorials"]
tags: ["prediction", "machine learning"]
subtitle: ''
summary: ''
authors: []
lastmod: '2022-04-22T16:12:58+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
    number_sections: true
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#learn-by-coding"><span class="toc-section-number">2</span> Learn by coding</a></li>
<li><a href="#tldr-just-give-me-the-code"><span class="toc-section-number">3</span> TL;DR, just give me the code!</a></li>
<li><a href="#other-resources"><span class="toc-section-number">4</span> Other resources</a></li>
</ul>
</div>

<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>Cross-entropy quantifies the difference between two probability distributions.
As such, it comes in handy as a <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a> in multi-class classification tasks (e.g., multinomial logistic regression).
Cross-entropy also provides an elegant solution for determining the difference between actual and predicted categorical data point values.</p>
</div>
<div id="learn-by-coding" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Learn by coding</h1>
<p>Bla bla</p>
<pre class="r"><code># Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin &lt;- ToothGrowth
dat_cat &lt;- iris</code></pre>
<p>First, binary cross entropy</p>
<pre class="r"><code># Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log &lt;- glm(supp ~ .,
               family = binomial(link = &#39;logit&#39;),
               data = ToothGrowth)

# Predictions
preds_log &lt;- predict(glm_log, type = &quot;response&quot;)

# Compute entropy
p &lt;- as.numeric(ToothGrowth$supp)-1 #
phat &lt;- preds_log

x &lt;- 0
for (i in 1:length(p)){
  x &lt;- x + (p[i] * log(phat[i]))
}

- x</code></pre>
<pre><code>##        1 
## 17.99278</code></pre>
<p>Then</p>
<pre class="r"><code># Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln &lt;- multinom(Species ~ ., data = iris)</code></pre>
<pre><code>## # weights:  18 (10 variable)
## initial  value 164.791843 
## iter  10 value 16.177348
## iter  20 value 7.111438
## iter  30 value 6.182999
## iter  40 value 5.984028
## iter  50 value 5.961278
## iter  60 value 5.954900
## iter  70 value 5.951851
## iter  80 value 5.950343
## iter  90 value 5.949904
## iter 100 value 5.949867
## final  value 5.949867 
## stopped after 100 iterations</code></pre>
<pre class="r"><code># Predictions
preds_mln &lt;- predict(glm_mln, type = &quot;probs&quot;)

# Compute entropy
p &lt;- FactoMineR::tab.disjonctif(iris$Species)
phat &lt;- preds_mln

x &lt;- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x &lt;- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)</code></pre>
<pre><code>## [1] 17.8496</code></pre>
<p>Finally</p>
<pre class="r"><code># Fast way to compute both -----------------------------------------------------

ce &lt;- - sum(
  diag(
    p %*% t(log(phat))
  )
)</code></pre>
</div>
<div id="tldr-just-give-me-the-code" class="section level1" number="3">
<h1><span class="header-section-number">3</span> TL;DR, just give me the code!</h1>
<pre class="r"><code># Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin &lt;- ToothGrowth
dat_cat &lt;- iris

# Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log &lt;- glm(supp ~ .,
               family = binomial(link = &#39;logit&#39;),
               data = ToothGrowth)

# Predictions
preds_log &lt;- predict(glm_log, type = &quot;response&quot;)

# Compute entropy
p &lt;- as.numeric(ToothGrowth$supp)-1 #
phat &lt;- preds_log

x &lt;- 0
for (i in 1:length(p)){
  x &lt;- x + (p[i] * log(phat[i]))
}

- x

# Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln &lt;- multinom(Species ~ ., data = iris)

# Predictions
preds_mln &lt;- predict(glm_mln, type = &quot;probs&quot;)

# Compute entropy
p &lt;- FactoMineR::tab.disjonctif(iris$Species)
phat &lt;- preds_mln

x &lt;- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x &lt;- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)

# Fast way to compute both -----------------------------------------------------

ce &lt;- - sum(
  diag(
    p %*% t(log(phat))
  )
)</code></pre>
</div>
<div id="other-resources" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Other resources</h1>
<ul>
<li><a href="https://rpubs.com/juanhklopper/cross_entropy">Cross-entropy in RPubs</a></li>
<li><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">A Gentle Introduction to Cross-Entropy for Machine Learning</a></li>
<li><a href="https://gombru.github.io/2018/05/23/cross_entropy_loss/">Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names</a></li>
</ul>
</div>
