---
title: Cross-entropy as a measure of predictive accuracy
draft: false # true
author: Edoardo Costantini
date: '2022-04-22'
slug: cross-entropy
categories: ["Tutorials", "Drafts"]
tags: ["prediction", "machine learning"]
subtitle: ''
summary: ''
authors: []
lastmod: '2022-04-22T16:12:58+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
    number_sections: true
---

# Introduction

Cross-entropy quantifies the difference between two probability distributions.
As such, it comes in handy as a [loss function](https://en.wikipedia.org/wiki/Loss_function) in multi-class classification tasks (e.g., multinomial logistic regression).
Cross-entropy also provides an elegant solution for determining the difference between actual and predicted categorical data point values.

# Learn by coding

Bla bla

```{r data, warning = FALSE}
# Prepare environment ----------------------------------------------------------

# Packages
library(nnet)

# Data
dat_bin <- ToothGrowth
dat_cat <- iris

```

First, binary cross entropy

```{r binary cross entropy}
# Binary cross entropy ---------------------------------------------------------

# Fit model
glm_log <- glm(supp ~ .,
               family = binomial(link = 'logit'),
               data = ToothGrowth)

# Predictions
preds_log <- predict(glm_log, type = "response")

# Compute entropy
p <- as.numeric(ToothGrowth$supp)-1 #
phat <- preds_log

x <- 0
for (i in 1:length(p)){
  x <- x + (p[i] * log(phat[i]))
}

- x

```

Then

```{r categorical cross entropy}
# Multi-categorical cross entropy ----------------------------------------------

# Fit model
glm_mln <- multinom(Species ~ ., data = iris)

# Predictions
preds_mln <- predict(glm_mln, type = "probs")

# Compute entropy
p <- FactoMineR::tab.disjonctif(iris$Species)
phat <- preds_mln

x <- 0
for (i in 1:nrow(p)){
  for (j in 1:ncol(p)){
    x <- x + (p[i, ] * log(phat[i, ]))
  }
}

- sum(x)

```

Finally

```{r fast way to compute it}
# Fast way to compute both -----------------------------------------------------

ce <- - sum(
  diag(
    p %*% t(log(phat))
  )
)

```


# TL;DR, just give me the code!
```{r TLDR, ref.label = knitr::all_labels(), echo=TRUE, eval=FALSE}
```

# Other resources

- [Cross-entropy in RPubs](https://rpubs.com/juanhklopper/cross_entropy)
- [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
- [Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names](https://gombru.github.io/2018/05/23/cross_entropy_loss/)
